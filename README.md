# Distributed Computation with Dask (Python)

This project is a **clean, productionâ€‘grade example** of how to build a **distributed Python application** using **Dask Distributed**, with:

- a **master** node (task submission + database writes)
- multiple **worker** nodes (parallel computation)
- a **shared Python package** (no import hacks, no `sys.path` tricks)
- a **local SQLite database** owned by the master
- full compatibility with **Windows + Linux**

If you followed the steps correctly, this setup:

âœ” avoids `ModuleNotFoundError`
âœ” avoids Dask deserialization errors
âœ” avoids environment mismatches
âœ” scales from 1 machine to many

---

## ğŸ§  Architecture Overview

```
                +--------------------+
                |   Dask Scheduler   |
                |  (task routing)    |
                +----------+---------+
                           |
          -----------------------------------------
          |                                       |
+--------------------+               +--------------------+
|   Worker Node 1    |               |   Worker Node N    |
|  ping computation |               |  ping computation |
+--------------------+               +--------------------+
                           |
                   +-------+-------+
                   |   Master Node |
                   | task submit + |
                   | SQLite write |
                   +---------------+
```

**Important rule:**
> Workers compute. The master writes to the database.

This avoids SQLite concurrency issues and keeps the system stable.

---

## ğŸ“ Project Layout

```
distributed_computation/
â”œâ”€ pyproject.toml
â”œâ”€ requirements.txt
â””â”€ src/
   â””â”€ distributed_computation/
      â”œâ”€ __init__.py
      â”‚
      â”œâ”€ common/
      â”‚  â”œâ”€ __init__.py
      â”‚  â””â”€ ping.py          # shared ping logic
      â”‚
      â”œâ”€ worker/
      â”‚  â”œâ”€ __init__.py
      â”‚  â””â”€ tasks.py         # Dask worker tasks
      â”‚
      â””â”€ master/
         â”œâ”€ __init__.py
         â”œâ”€ db_writer.py     # SQLite writer
         â””â”€ run_master.py   # entry point
```

### Why this layout matters

- The project is a **real Python package**
- It can be **installed with pip**
- Dask workers can **import code safely**
- No relative imports, no fragile hacks

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Create a virtual environment (recommended)

```bash
python -m venv .venv
source .venv/bin/activate   # Linux / macOS
.venv\Scripts\activate      # Windows
```

### 2ï¸âƒ£ Install the project (editable mode)

```bash
pip install -e .
```

This step is **mandatory** on:
- the scheduler machine
- every worker machine
- the master machine

> If a worker cannot `pip install` your code, Dask cannot run it.

---

## ğŸš€ Running the System

### 1ï¸âƒ£ Start the Dask scheduler

```bash
dask scheduler
```

You should see:

```
Dashboard at: http://127.0.0.1:8787/status
```

---

### 2ï¸âƒ£ Start one or more workers

On **any machine** (same network):

```bash
dask worker tcp://<scheduler-ip>:8786
```

Optional tuning:

```bash
dask worker tcp://<scheduler-ip>:8786 --nthreads 16 --memory-limit 8GB
```

To prevent Ubuntu client from sleeping : 
```bash
systemd-inhibit --what=sleep:idle --why="Dask worker running" dask worker tcp://<scheduler-ip>:8786 --nthreads 16 --memory-limit 8GB

---

### 3ï¸âƒ£ Run the master

```bash
python -m distributed_computation.master.run_master \
  --scheduler tcp://<scheduler-ip>:8786 \
  --db hosts.db \
  --hosts hosts.txt
```

The master:
- loads hosts
- submits tasks to workers
- batches results
- writes to SQLite

---

## ğŸ—„ Database Design

- **SQLite** is used for simplicity
- Only the **master writes** to the database
- Workers never touch the DB

Schema:

```sql
CREATE TABLE hosts (
  host TEXT PRIMARY KEY,
  status TEXT
);
```

Batch inserts are used for performance.

---

## ğŸ“Š Dask Dashboard

The Dask dashboard provides:

- task progress
- worker CPU / memory
- task stream
- scheduler health

### Dependency

The dashboard requires:

```
bokeh>=3.1.0
```

This dependency is declared in `pyproject.toml` and installed automatically.

Open the dashboard:

```
http://<scheduler-ip>:8787/status
```

---

## ğŸ”’ Import & Serialization Rules (Very Important)

### âœ… Do

- Use **absolute imports**
- Install the package with `pip install -e .`
- Submit **functions**, not strings, to Dask

```python
client.map(ping_task, hosts)
```

### âŒ Never do

- `sys.path.append(...)`
- relative imports across packages
- `client.map("module.function", ...)`
- running files directly (`python file.py`)

---

## ğŸ§  Key Lessons Learned

- Dask workers are **independent Python processes**
- They do **not** inherit your working directory
- Code must be **installed**, not assumed
- Packaging correctly solves 95% of Dask errors

---

## ğŸ›  Troubleshooting

### Dashboard does not load

âœ” Check `bokeh>=3.1.0` is installed
âœ” Restart scheduler after installing

---

### `ModuleNotFoundError` on workers

âœ” Package installed on worker machine
âœ” Same Python version
âœ” Same virtual environment

---

### SQLite locking issues

âœ” Ensure only master writes
âœ” Increase batch size

---

## ğŸš§ Next Steps / Improvements

- PostgreSQL instead of SQLite (multiâ€‘writer)
- Docker / dockerâ€‘compose
- TLSâ€‘secured scheduler
- Task retries & backpressure
- Async result handling
- Metrics & logging

---

## ğŸ Final Note

This project demonstrates the **correct way** to structure and run a distributed Python application with Dask.

Once your code is:

âœ” installable
âœ” importable
âœ” environmentâ€‘consistent

Dask becomes boring â€” and thatâ€™s a good thing.

Happy distributed computing ğŸš€

